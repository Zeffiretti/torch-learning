{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convolution network\n",
    "\n",
    "## description in textbook\n",
    "\n",
    "- continous form\n",
    "  \n",
    "$\\begin{equation*}\n",
    "(f*g)(n)=∫_{-∞}^{∞}f{τ}g(n-τ)dτ\n",
    "\\end{equation*}$\n",
    "\n",
    "- discrete from\n",
    "\n",
    "$\\begin{equation*}\n",
    "(f*g)(n)=∑_{τ=-∞}^{∞}f{τ}g(n-τ)\n",
    "\\end{equation*}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对卷积这个名词的理解：**所谓两个函数的卷积，本质上就是先将一个函数翻转，然后进行滑动叠加。**\n",
    "\n",
    "参考[知乎](https://www.zhihu.com/question/22298352)回答。\n",
    "\n",
    "**瞬时行为的持续性后果**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding=0 torch.Size([1, 3, 28, 28])\n",
      "padding=1 torch.Size([1, 3, 28, 28])\n",
      "convenient torch.Size([1, 3, 28, 28])\n",
      "layer.weight Parameter containing:\n",
      "tensor([[[[-0.1566,  0.1914, -0.1275],\n",
      "          [ 0.1642, -0.1769,  0.1793],\n",
      "          [ 0.0542, -0.1038, -0.1445]],\n",
      "\n",
      "         [[-0.1629,  0.0872, -0.0846],\n",
      "          [-0.1031, -0.0091,  0.0808],\n",
      "          [-0.0332, -0.1247,  0.0483]],\n",
      "\n",
      "         [[ 0.0423,  0.1400,  0.0646],\n",
      "          [-0.0966,  0.0957,  0.1444],\n",
      "          [-0.1827,  0.0433,  0.0251]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1260,  0.1110,  0.0495],\n",
      "          [-0.1100,  0.1883,  0.1512],\n",
      "          [-0.0388,  0.0369, -0.1890]],\n",
      "\n",
      "         [[-0.1467, -0.0309, -0.1235],\n",
      "          [-0.1162,  0.1120,  0.0808],\n",
      "          [ 0.0552, -0.1084, -0.0282]],\n",
      "\n",
      "         [[-0.1608, -0.0328,  0.0395],\n",
      "          [ 0.1226,  0.0333,  0.0160],\n",
      "          [ 0.0095,  0.1576,  0.0935]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1093, -0.0179,  0.0214],\n",
      "          [-0.1504, -0.0490, -0.1740],\n",
      "          [ 0.0930,  0.1673,  0.1551]],\n",
      "\n",
      "         [[-0.0974,  0.1509,  0.1590],\n",
      "          [ 0.1287,  0.1740, -0.0302],\n",
      "          [ 0.1327, -0.1014,  0.1762]],\n",
      "\n",
      "         [[-0.0708,  0.1096, -0.0450],\n",
      "          [-0.1070,  0.0930, -0.0284],\n",
      "          [ 0.0390,  0.1042, -0.0040]]]], requires_grad=True)\n",
      "layer.bias Parameter containing:\n",
      "tensor([ 0.0054, -0.0992, -0.1683], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 2d convolution\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "x = torch.rand(1, 3, 28, 28)\n",
    "# parameters 1st: 3, input has 3 channels\n",
    "# parameters 2nd: 3, output has 3 channels\n",
    "layer = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "out = layer.forward(x)\n",
    "print(\"padding=0\", out.size())\n",
    "\n",
    "# [b,1,28,28] == kernel[3,1,3,3] ==> [1,2,28,28]\n",
    "out = layer.forward(x)\n",
    "print(\"padding=1\", out.size())\n",
    "\n",
    "# convenient way\n",
    "out = layer(x)  #.__call__\n",
    "print(\"convenient\", out.size())\n",
    "\n",
    "# some information\n",
    "print(\"layer.weight\", layer.weight)\n",
    "print(\"layer.bias\", layer.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out torch.Size([1, 16, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "# low-level usage\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "x = torch.rand(1, 3, 28, 28)\n",
    "w = torch.rand(16, 3, 5, 5)\n",
    "b = torch.rand(16)\n",
    "out = F.conv2d(x, w, b, stride=1, padding=1)\n",
    "print(\"out\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling & Subsampling\n",
    "\n",
    "## Batch Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.running_mean tensor([0.0499, 0.0500, 0.0501, 0.0501, 0.0500, 0.0500, 0.0499, 0.0499, 0.0501,\n",
      "        0.0499, 0.0499, 0.0499, 0.0502, 0.0499, 0.0501, 0.0500])\n",
      "layer.running_var tensor([0.9083, 0.9083, 0.9084, 0.9083, 0.9083, 0.9083, 0.9084, 0.9084, 0.9084,\n",
      "        0.9083, 0.9084, 0.9083, 0.9083, 0.9083, 0.9083, 0.9084])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "x = torch.rand(100, 16, 784)  # 28*28\n",
    "layer = nn.BatchNorm1d(16)\n",
    "out = layer(x)\n",
    "print(\"layer.running_mean\", layer.running_mean)\n",
    "print(\"layer.running_var\", layer.running_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class ResBlk(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            ch_in, ch_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch_out)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            ch_in, ch_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(ch_out)\n",
    "\n",
    "        self.extra = nn.Sequential()\n",
    "        if ch_out != ch_in:\n",
    "            # [b,ch_in,h,w] ==> [b,ch_out,h,w]\n",
    "            self.extra = nn.Sequential(\n",
    "                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1),\n",
    "                nn.BatchNorm2d(ch_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.extra(x) + out\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **nn.Module** 模块\n",
    "`nn.Module` 是所有网络层的父类，即所有自己实现的和官方已经实现的网络层均需继承自`nn.Moudle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear layer example\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, inp, outp):\n",
    "        super(MyLinear, self).__init__()\n",
    "        # requires_grad = True\n",
    "        self.w = nn.Parameter(torch.randn(outp, inp))\n",
    "        self.b = nn.Parameter(torch.randn(outp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x@self.w.t()+self.b\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magic\n",
    "- Every Layer is `nn.Module`\n",
    "  - `nn.Linear`\n",
    "  - `nn.BatchNorm2d`\n",
    "  - `nn.Conv2s`\n",
    "- `nn.Module` nested in `nn.Module`\n",
    "\n",
    "1. embed current layers\n",
    "  - Linear\n",
    "  - ReLu\n",
    "  - Sigmoid\n",
    "  - Conv2d\n",
    "  - ConvTransposed2d\n",
    "  - Dropout\n",
    "  - etc.\n",
    "2. container\n",
    "  - net(x)\n",
    "  ```python\n",
    "  self.net = nn.Sequential(\n",
    "    nn.Conv2d(1,32,5,1,1),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.RelU(True),\n",
    "    nn.BatchNorm2s(32),\n",
    "\n",
    "    nn.Conv2d(32,63,3,1,1),\n",
    "    nn.RelU(True),\n",
    "    nn.BatchNorm2s(64),\n",
    "\n",
    "    nn.Conv2d(64,64,3,1,1),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.RelU(True),\n",
    "    nn.BatchNorm2s(64),\n",
    "\n",
    "    nn.Conv2d(64,128,3,1,1),\n",
    "    nn.RelU(True),\n",
    "    nn.BatchNorm2s(128),\n",
    "  )\n",
    "  ```\n",
    "3. parameters management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-linear-layer network has parameters of 4\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n",
      "('0.weight', Parameter containing:\n",
      "tensor([[ 0.1828,  0.2472, -0.3861, -0.0761],\n",
      "        [ 0.3324, -0.3383,  0.0449,  0.1659]], requires_grad=True))\n",
      "('0.bias', Parameter containing:\n",
      "tensor([0.2517, 0.3928], requires_grad=True))\n",
      "dict_items([('0.weight', Parameter containing:\n",
      "tensor([[ 0.1828,  0.2472, -0.3861, -0.0761],\n",
      "        [ 0.3324, -0.3383,  0.0449,  0.1659]], requires_grad=True)), ('0.bias', Parameter containing:\n",
      "tensor([0.2517, 0.3928], requires_grad=True)), ('1.weight', Parameter containing:\n",
      "tensor([[ 0.1360, -0.6089],\n",
      "        [ 0.4348,  0.4154]], requires_grad=True)), ('1.bias', Parameter containing:\n",
      "tensor([ 0.2304, -0.1044], requires_grad=True))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 2), nn.Linear(2, 2))\n",
    "print(\"2-linear-layer network has parameters of\",\n",
    "      len(list(net.parameters())))  # 4 parameters group in total\n",
    "for i in range(len(list(net.parameters()))):\n",
    "    print(list(net.parameters())[i].shape)\n",
    "\n",
    "print(list(net.named_parameters())[0])\n",
    "print(list(net.named_parameters())[1])\n",
    "print(dict(net.named_parameters()).items())\n",
    "\n",
    "# the way to iterate parameters\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. modules\n",
    "  - modules: all nodes\n",
    "  - children: direct children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.net = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(BasicNet(),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(3, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. to(device)<br>\n",
    "example:   \n",
    "   ```python\n",
    "   device=torch.device('cuda')\n",
    "   net=Net()\n",
    "   net.to(device)\n",
    "   ```\n",
    "\n",
    "6. save and load\n",
    "   ```python\n",
    "   device=torch.device('cuda')\n",
    "   net=Net()\n",
    "   net.to(device)\n",
    "\n",
    "   net.load_state_dict(torch.load('ckpt.mdl'))\n",
    "\n",
    "   # train...\n",
    "\n",
    "   torch.save(net.state_dict(), 'cpkt.mdl')\n",
    "   ```\n",
    "\n",
    "7. train/test\n",
    "   ```python\n",
    "   device=torch.device('cuda')\n",
    "   net=Net()\n",
    "   net.to(device)\n",
    "\n",
    "   # train\n",
    "   net.train()\n",
    "\n",
    "   # test\n",
    "   net.eval()\n",
    "   ```\n",
    "\n",
    "8. implement own layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNet, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1, 16, stride=1, padding=1),\n",
    "                                 nn.MaxPool2d(2, 2),\n",
    "                                 Flatten(),\n",
    "                                 nn.Linear(1*14*14, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data argumentation\n",
    "### Big Data\n",
    "- the key to prevent overfitting\n",
    "##### sample more data?\n",
    "- consuming\n",
    "##### limited data\n",
    "- small network capacity\n",
    "- regularization\n",
    "- data argumentation\n",
    "### Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "cifar_train = datasets.CIFAR10(\n",
    "    'cifar', True, transform=transforms.Compose([transforms.Resize(32, 32), transforms.ToTensor()]), download=True)\n",
    "cifar_train = DataLoader(cifar_train, batch_size=10, shuffle=True)\n",
    "\n",
    "cifar_test = datasets.CIFAR10(\n",
    "    'cifar', False, transform=transforms.Compose([transforms.Resize(32, 32), transforms.ToTensor()]), download=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20dd1f611441082596deb3b541c80de01b5ee275335c977fc3f86eeae6739934"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

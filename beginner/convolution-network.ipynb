{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# convolution network\n",
    "\n",
    "## description in textbook\n",
    "\n",
    "- continous form\n",
    "  \n",
    "$\\begin{equation*}\n",
    "(f*g)(n)=∫_{-∞}^{∞}f{τ}g(n-τ)dτ\n",
    "\\end{equation*}$\n",
    "\n",
    "- discrete from\n",
    "\n",
    "$\\begin{equation*}\n",
    "(f*g)(n)=∑_{τ=-∞}^{∞}f{τ}g(n-τ)\n",
    "\\end{equation*}$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "对卷积这个名词的理解：**所谓两个函数的卷积，本质上就是先将一个函数翻转，然后进行滑动叠加。**\n",
    "\n",
    "参考[知乎](https://www.zhihu.com/question/22298352)回答。\n",
    "\n",
    "**瞬时行为的持续性后果**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# 2d convolution\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "x = torch.rand(1, 3, 28, 28)\n",
    "# parameters 1st: 3, input has 3 channels\n",
    "# parameters 2nd: 3, output has 3 channels\n",
    "layer = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "out = layer.forward(x)\n",
    "print(\"padding=0\", out.size())\n",
    "\n",
    "# [b,1,28,28] == kernel[3,1,3,3] ==> [1,2,28,28]\n",
    "out = layer.forward(x)\n",
    "print(\"padding=1\", out.size())\n",
    "\n",
    "# convenient way\n",
    "out = layer(x)  #.__call__\n",
    "print(\"convenient\", out.size())\n",
    "\n",
    "# some information\n",
    "print(\"layer.weight\", layer.weight)\n",
    "print(\"layer.bias\", layer.bias)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "padding=0 torch.Size([1, 3, 28, 28])\n",
      "padding=1 torch.Size([1, 3, 28, 28])\n",
      "convenient torch.Size([1, 3, 28, 28])\n",
      "layer.weight Parameter containing:\n",
      "tensor([[[[-0.0315,  0.0885,  0.0659],\n",
      "          [ 0.0216, -0.1016,  0.0011],\n",
      "          [ 0.1141, -0.0933, -0.1203]],\n",
      "\n",
      "         [[-0.1121,  0.0020,  0.1166],\n",
      "          [-0.0271,  0.0315, -0.0622],\n",
      "          [ 0.1611,  0.0671,  0.1478]],\n",
      "\n",
      "         [[ 0.1047, -0.0065,  0.1638],\n",
      "          [ 0.0945, -0.1282, -0.0828],\n",
      "          [ 0.1281, -0.1003, -0.0865]]],\n",
      "\n",
      "\n",
      "        [[[-0.1422, -0.1166, -0.0918],\n",
      "          [-0.0167,  0.1351, -0.1817],\n",
      "          [-0.0331,  0.1771, -0.1429]],\n",
      "\n",
      "         [[ 0.1405,  0.0679, -0.0774],\n",
      "          [-0.1561, -0.0875, -0.0400],\n",
      "          [-0.0537,  0.0367, -0.1841]],\n",
      "\n",
      "         [[-0.1595,  0.0247, -0.1373],\n",
      "          [ 0.1016,  0.0223,  0.0657],\n",
      "          [-0.0454,  0.1220, -0.1142]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0137, -0.0186, -0.0910],\n",
      "          [ 0.1029,  0.1110,  0.1907],\n",
      "          [ 0.0869,  0.0613,  0.1082]],\n",
      "\n",
      "         [[ 0.1158, -0.1820, -0.1821],\n",
      "          [-0.1748,  0.0913,  0.0057],\n",
      "          [-0.0732, -0.0922, -0.1597]],\n",
      "\n",
      "         [[-0.0421,  0.0906,  0.0028],\n",
      "          [-0.0362,  0.1730, -0.0066],\n",
      "          [ 0.0823,  0.0626,  0.0662]]]], requires_grad=True)\n",
      "layer.bias Parameter containing:\n",
      "tensor([-0.0092, -0.1460, -0.0387], requires_grad=True)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# low-level usage\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "x = torch.rand(1, 3, 28, 28)\n",
    "w = torch.rand(16, 3, 5, 5)\n",
    "b = torch.rand(16)\n",
    "out = F.conv2d(x, w, b, stride=1, padding=1)\n",
    "print(\"out\", out.shape)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "out torch.Size([1, 16, 26, 26])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Max pooling & Subsampling\n",
    "\n",
    "## Batch Normalization\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "x = torch.rand(100, 16, 784)  # 28*28\n",
    "layer = nn.BatchNorm1d(16)\n",
    "out = layer(x)\n",
    "print(\"layer.running_mean\", layer.running_mean)\n",
    "print(\"layer.running_var\", layer.running_var)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "layer.running_mean tensor([0.0499, 0.0499, 0.0502, 0.0501, 0.0500, 0.0501, 0.0502, 0.0499, 0.0499,\n",
      "        0.0500, 0.0502, 0.0500, 0.0499, 0.0501, 0.0499, 0.0501])\n",
      "layer.running_var tensor([0.9083, 0.9083, 0.9083, 0.9083, 0.9083, 0.9084, 0.9084, 0.9083, 0.9083,\n",
      "        0.9083, 0.9084, 0.9083, 0.9083, 0.9083, 0.9084, 0.9083])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet Implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class ResBlk(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            ch_in, ch_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch_out)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            ch_in, ch_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(ch_out)\n",
    "\n",
    "        self.extra = nn.Sequential()\n",
    "        if ch_out != ch_in:\n",
    "            # [b,ch_in,h,w] ==> [b,ch_out,h,w]\n",
    "            self.extra = nn.Sequential(\n",
    "                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1),\n",
    "                nn.BatchNorm2d(ch_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.extra(x) + out\n",
    "        return out\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **nn.Module** 模块\n",
    "`nn.Module` 是所有网络层的父类，即所有自己实现的和官方已经实现的网络层均需继承自`nn.Moudle`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# linear layer example\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, inp, outp):\n",
    "        super(MyLinear, self).__init__()\n",
    "        # requires_grad = True\n",
    "        self.w = nn.Parameter(torch.randn(outp, inp))\n",
    "        self.b = nn.Parameter(torch.randn(outp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x@self.w.t()+self.b\n",
    "        return x\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Magic\n",
    "- Every Layer is `nn.Module`\n",
    "  - `nn.Linear`\n",
    "  - `nn.BatchNorm2d`\n",
    "  - `nn.Conv2s`\n",
    "- `nn.Module` nested in `nn.Module`\n",
    "\n",
    "1. embed current layers\n",
    "  - Linear\n",
    "  - ReLu\n",
    "  - Sigmoid\n",
    "  - Conv2d\n",
    "  - ConvTransposed2d\n",
    "  - Dropout\n",
    "  - etc.\n",
    "2. container\n",
    "  - net(x)\n",
    "  ```python\n",
    "  self.net = nn.Sequential(\n",
    "    nn.Conv2d(1,32,5,1,1),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.RelU(True),\n",
    "    nn.BatchNorm2s(32),\n",
    "\n",
    "    nn.Conv2d(32,63,3,1,1),\n",
    "    nn.RelU(True),\n",
    "    nn.BatchNorm2s(64),\n",
    "\n",
    "    nn.Conv2d(64,64,3,1,1),\n",
    "    nn.MaxPool2d(2,2),\n",
    "    nn.RelU(True),\n",
    "    nn.BatchNorm2s(64),\n",
    "\n",
    "    nn.Conv2d(64,128,3,1,1),\n",
    "    nn.RelU(True),\n",
    "    nn.BatchNorm2s(128),\n",
    "  )\n",
    "  ```\n",
    "3. parameters management"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 2), nn.Linear(2, 2))\n",
    "print(\"2-linear-layer network has parameters of\",\n",
    "      len(list(net.parameters())))  # 4 parameters group in total\n",
    "for i in range(len(list(net.parameters()))):\n",
    "    print(list(net.parameters())[i].shape)\n",
    "\n",
    "print(list(net.named_parameters())[0])\n",
    "print(list(net.named_parameters())[1])\n",
    "print(dict(net.named_parameters()).items())\n",
    "\n",
    "# the way to iterate parameters\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2-linear-layer network has parameters of 4\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n",
      "('0.weight', Parameter containing:\n",
      "tensor([[ 0.1828,  0.2472, -0.3861, -0.0761],\n",
      "        [ 0.3324, -0.3383,  0.0449,  0.1659]], requires_grad=True))\n",
      "('0.bias', Parameter containing:\n",
      "tensor([0.2517, 0.3928], requires_grad=True))\n",
      "dict_items([('0.weight', Parameter containing:\n",
      "tensor([[ 0.1828,  0.2472, -0.3861, -0.0761],\n",
      "        [ 0.3324, -0.3383,  0.0449,  0.1659]], requires_grad=True)), ('0.bias', Parameter containing:\n",
      "tensor([0.2517, 0.3928], requires_grad=True)), ('1.weight', Parameter containing:\n",
      "tensor([[ 0.1360, -0.6089],\n",
      "        [ 0.4348,  0.4154]], requires_grad=True)), ('1.bias', Parameter containing:\n",
      "tensor([ 0.2304, -0.1044], requires_grad=True))])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. modules\n",
    "  - modules: all nodes\n",
    "  - children: direct children"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.net = nn.Linear(4, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(BasicNet(),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(3, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. to(device)<br>\n",
    "example:   \n",
    "   ```python\n",
    "   device=torch.device('cuda')\n",
    "   net=Net()\n",
    "   net.to(device)\n",
    "   ```\n",
    "\n",
    "6. save and load\n",
    "   ```python\n",
    "   device=torch.device('cuda')\n",
    "   net=Net()\n",
    "   net.to(device)\n",
    "\n",
    "   net.load_state_dict(torch.load('ckpt.mdl'))\n",
    "\n",
    "   # train...\n",
    "\n",
    "   torch.save(net.state_dict(), 'cpkt.mdl')\n",
    "   ```\n",
    "\n",
    "7. train/test\n",
    "   ```python\n",
    "   device=torch.device('cuda')\n",
    "   net=Net()\n",
    "   net.to(device)\n",
    "\n",
    "   # train\n",
    "   net.train()\n",
    "\n",
    "   # test\n",
    "   net.eval()\n",
    "   ```\n",
    "\n",
    "8. implement own layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestNet, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1, 16, stride=1, padding=1),\n",
    "                                 nn.MaxPool2d(2, 2),\n",
    "                                 Flatten(),\n",
    "                                 nn.Linear(1*14*14, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## data argumentation\n",
    "### Big Data\n",
    "- the key to prevent overfitting\n",
    "##### sample more data?\n",
    "- consuming\n",
    "##### limited data\n",
    "- small network capacity\n",
    "- regularization\n",
    "- data argumentation\n",
    "### Recap"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "cifar_train = datasets.CIFAR10(\n",
    "    'cifar', True, transform=transforms.Compose([transforms.Resize(32, 32), transforms.ToTensor()]), download=True)\n",
    "cifar_train = DataLoader(cifar_train, batch_size=10, shuffle=True)\n",
    "\n",
    "cifar_test = datasets.CIFAR10(\n",
    "    'cifar', False, transform=transforms.Compose([transforms.Resize(32, 32), transforms.ToTensor()]), download=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "a8f600b2f06634c75376a3539a1f60621875efab213eb41935f5c979bfebb892"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}